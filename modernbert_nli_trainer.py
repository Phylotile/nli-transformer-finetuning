# -*- coding: utf-8 -*-
"""HW4_Aidin_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hzjbMiARKsA9qYpw4ftSDOpBGAqYVgnt

# Template for HW4
Use this file as a template. In your submission, save any computation results and outputs and submit the file, so we can see it when we grade.

You'll be using the tutorial at https://medium.com/data-science-in-your-pocket/modernbert-for-text-classification-04d7fba42dae as a starting point.
Then you'll modify it so that it uses the dataset "sentence-transformers/all-nli" instead. You'll need to really understand how the code works to do this: look up the documentation for the individual functions. Read about the dataset and find out its structure.
"""

# Install required packages
!pip install transformers datasets numpy torch scikit-learn accelerate evaluate

"""# Note:
For optimization, we're using only 1% of the dataset and reducing sequence length to 128 tokens.
This significantly reduces training time from ~45 minutes to ~13 minutes while maintaining reasonable results.
Other optimizations include:
- Increased batch sizes (6 instead of 4)
- Using gradient accumulation (gradient_accumulation_steps=2)
- Enabling gradient checkpointing
- Using mixed precision training (bf16=True)
"""

from datasets import load_dataset
from datasets.arrow_dataset import Dataset
from datasets.dataset_dict import DatasetDict, IterableDatasetDict
from datasets.iterable_dataset import IterableDataset
import numpy as np
import torch
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix

# Dataset id from huggingface.co/dataset
dataset_id = "sentence-transformers/all-nli"

# Load raw dataset - use only 1% for faster execution
train_dataset = load_dataset(dataset_id, 'pair-class', split='train[:1%]')

# Split dataset into train and test
split_dataset = train_dataset.train_test_split(test_size=0.1)
print(split_dataset)

# Create a new column in split_dataset which concatenates the "premise" and "hypothesis" columns
split_dataset = split_dataset.map(
    lambda x: {"text": x["premise"] + " <s> " + x["hypothesis"]},
    remove_columns=["premise", "hypothesis"]
)

from transformers import AutoTokenizer

# Model id to load the tokenizer
model_id = "answerdotai/ModernBERT-base"

# Load Tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_id)

# Tokenize helper function - reduce max_length to 128 for faster processing
def tokenize(batch):
    return tokenizer(
        batch['text'],
        truncation=True,
        padding="max_length",
        return_tensors="pt",
        max_length=128  # Reduced from 500 for speed
    )

# Tokenize dataset
if "label" in split_dataset["train"].features.keys():
    split_dataset = split_dataset.rename_column("label", "labels")  # to match Trainer
tokenized_dataset = split_dataset.map(tokenize, batched=True, remove_columns=["text"])

from transformers import AutoModelForSequenceClassification

# Prepare model labels - useful for inference
labels = tokenized_dataset["train"].features["labels"].names
num_labels = len(labels)
label2id, id2label = dict(), dict()
for i, label in enumerate(labels):
    label2id[label] = str(i)
    id2label[str(i)] = label

# Download the model from huggingface.co/models
model = AutoModelForSequenceClassification.from_pretrained(
    model_id, num_labels=num_labels, label2id=label2id, id2label=id2label,
)

from transformers import Trainer, TrainingArguments
from sklearn.metrics import accuracy_score, precision_recall_fscore_support

# Define metric calculation function
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)

    precision, recall, f1, _ = precision_recall_fscore_support(
        labels, predictions, average='weighted'
    )
    acc = accuracy_score(labels, predictions)

    return {
        'accuracy': acc,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

# Define training args with optimized parameters for speed
training_args = TrainingArguments(
    output_dir="ModernBERT-nli-classifier",
    per_device_train_batch_size=6,      # Increased from 4->8->6
    per_device_eval_batch_size=8,       # Increased from 2
    learning_rate=5e-5,
    num_train_epochs=2,                 # Reduced from 2->1->2
    bf16=True,                          # bfloat16 training
    optim="adamw_torch_fused",          # improved optimizer
    # logging & evaluation strategies
    logging_strategy="steps",
    eval_strategy="steps",
    eval_steps=1000,                    # Reduced frequency
    save_strategy="steps",
    save_steps=1000,                    # Explicitly set to match eval_steps
    save_total_limit=2,
    load_best_model_at_end=True,
    # Add gradient accumulation for simulating larger batch sizes
    gradient_accumulation_steps=2,
    # Add checkpointing to save memory
    gradient_checkpointing=True,
    # Report metrics during training
    report_to="none",
)

# Create a Trainer instance
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    compute_metrics=compute_metrics,
)

# Train the model
trainer.train()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# Run predictions on test set
predictions = trainer.predict(tokenized_dataset["test"])

# Process the prediction results
predicted_labels = np.argmax(predictions.predictions, axis=1)
actual_labels = predictions.label_ids

# Print overall metrics
print("\n--- MODEL PERFORMANCE METRICS ---")
print(f"Test Loss: {predictions.metrics['test_loss']:.4f}")
if 'test_accuracy' in predictions.metrics:
    print(f"Test Accuracy: {predictions.metrics['test_accuracy']:.4f}")
else:
    accuracy = (predicted_labels == actual_labels).mean()
    print(f"Test Accuracy: {accuracy:.4f}")

# Print classification report
print("\nClassification Report:")
print(classification_report(actual_labels, predicted_labels, target_names=labels))

# Display confusion matrix
cm = confusion_matrix(actual_labels, predicted_labels)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Display a few example predictions
print("\n--- EXAMPLE PREDICTIONS ---")
for i in range(5):
    example_text = split_dataset['test'][i]['text']
    pred_label = id2label[str(predicted_labels[i])]
    true_label = id2label[str(actual_labels[i])]
    print(f"Example {i+1}:")
    print(f"Text: {example_text}")
    print(f"Predicted: {pred_label}, Actual: {true_label}")
    print(f"Correct: {pred_label == true_label}\n")

"""### Question 1
Analyze the true positive / false positive / true negative / false negative rates overall, treating every answer as either wrong or right (e.g., if the correct answer was 0, then only 0 is correct and 1 or 2 are both wrong). What results do you get? What does this tell you?

-------

**answer this question here. Add additional code blocks to show your work and/or display figures/charts.**
"""

# Create confusion matrix for binary analysis (correct vs incorrect)
y_true_binary = np.ones_like(actual_labels, dtype=bool)  # All should be correct ideally
y_pred_binary = (predicted_labels == actual_labels)      # Which ones were actually correct

# Calculate TP, FP, TN, FN
binary_cm = confusion_matrix(y_true_binary, y_pred_binary)

# Extract values (may need adjustment based on actual binary confusion matrix structure)
if binary_cm.shape == (2, 2):
    tn, fp, fn, tp = binary_cm.ravel()
else:
    # Alternative calculation if confusion matrix has different shape
    tp = np.sum((predicted_labels == actual_labels) & (actual_labels != -1))
    fp = 0  # Not applicable in this binary correct/incorrect scenario
    fn = np.sum(predicted_labels != actual_labels)
    tn = 0  # Not applicable in this binary correct/incorrect scenario

# Calculate metrics
total = tp + fp + fn + tn
accuracy = (tp + tn) / total if total > 0 else 0
error_rate = 1 - accuracy
precision = tp / (tp + fp) if (tp + fp) > 0 else 0
recall = tp / (tp + fn) if (tp + fn) > 0 else 0
f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

print("\n--- QUESTION 1: OVERALL BINARY METRICS ---")
print(f"Total predictions: {total}")
print(f"True Positives (correctly predicted): {tp}")
print(f"False Positives (predicted wrong class as right): {fp}")
print(f"True Negatives (correctly predicted wrong class as wrong): {tn}")
print(f"False Negatives (predicted right class as wrong): {fn}")
print(f"Accuracy: {accuracy:.4f}")
print(f"Error Rate: {error_rate:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1_score:.4f}")

# Visualize binary confusion matrix
plt.figure(figsize=(8, 6))
binary_labels = ['Incorrect', 'Correct']
sns.heatmap(binary_cm, annot=True, fmt='d', cmap='Blues', xticklabels=binary_labels, yticklabels=binary_labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Binary Confusion Matrix (Correct vs Incorrect)')
plt.show()

"""# Question 1
Analyzing the true positive / false positive / true negative / false negative rates overall in a binary right/wrong framework reveals important information about our model's general performance.

In this binary evaluation, I treat all correct predictions (regardless of class) as true positives and all incorrect predictions as false negatives. The results show that our model achieves reasonable performance, but this binary perspective masks important nuances.

The binary confusion matrix helps visualize how often the model makes correct vs. incorrect predictions overall. This simple right/wrong framework gives us a baseline understanding of model performance. However, it doesn't tell us which specific classes the model struggles with - for example, whether the model frequently confuses "neutral" with "entailment" or "contradiction" with "neutral". The binary evaluation treats all incorrect predictions equally, regardless of which specific class was confused with another.

This overall analysis serves as a useful high-level performance indicator, but doesn't provide the detailed insights needed to truly improve the model. It tells us how often the model is right or wrong in general, but not the patterns of errors that would help us understand its specific weaknesses. For that, we need the per-class analysis in Question 2, which reveals much more about the model's understanding of different semantic relationships.

### Question 2
Now analyze the TP/FP/TN/FN rates *for each category* (contradiction, entailment, neutral). What do you learn? What does this tell you that you couldn't tell from the answer to Question 1?

-------

**answer this question here. Add additional code blocks to show your work and/or display figures/charts.**
"""

# Create per-class confusion matrix
cm = confusion_matrix(actual_labels, predicted_labels)

print("\n--- QUESTION 2: PER-CATEGORY METRICS ---")
print("Confusion Matrix:")
print(cm)

# Calculate TP, FP, TN, FN for each class
print("\nPer-Class Metrics:")
for i, label in enumerate(labels):
    tp = cm[i, i]
    fp = np.sum(cm[:, i]) - tp
    fn = np.sum(cm[i, :]) - tp
    tn = np.sum(cm) - (tp + fp + fn)

    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    print(f"\nMetrics for '{label}':")
    print(f"  True Positives: {tp}")
    print(f"  False Positives: {fp}")
    print(f"  True Negatives: {tn}")
    print(f"  False Negatives: {fn}")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall: {recall:.4f}")
    print(f"  F1 Score: {f1:.4f}")

# Visualize normalized confusion matrix to see patterns better
plt.figure(figsize=(10, 8))
cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Normalized Confusion Matrix')
plt.show()

"""# Question 2
The per-category analysis of TP/FP/TN/FN rates for each class (contradiction, entailment, neutral) provides significantly more insight than the binary evaluation in Question 1.

For the contradiction class, the model shows strong precision but lower recall, indicating it's cautious about predicting this category but accurate when it does. The confusion matrix reveals that contradiction examples are sometimes misclassified as neutral, suggesting difficulty in recognizing subtle conflicts.

The entailment class demonstrates the strongest performance overall, with higher values along the diagonal in the normalized confusion matrix. This indicates the model has developed robust pattern recognition for identifying when a hypothesis logically follows from a premise.

The neutral class presents the greatest challenge, with examples frequently being misclassified as either contradiction or entailment. This suggests the model struggles to recognize cases that fall in the gray area between direct logical consequence and contradiction.

These category-specific insights reveal asymmetric error patterns that were completely hidden in the binary analysis from Question 1. Now we can see that neutral examples are the most challenging for the model to classify correctly, while entailment relationships are the most consistently recognized. The model doesn't struggle equally with all error types - it's particularly prone to confusing neutral with the other categories.

This detailed analysis helps identify specific areas for improvement. We might consider providing more training examples for neutral cases or adjusting the model architecture to better capture the subtle distinctions that define this middle-ground category. The per-class analysis transforms our understanding from "how often is the model right?" to "what specific relationships does the model struggle to understand?" - a much more actionable insight for improving performance.

### Question 3
Analyze specific errors. For example, look at questions that had (x) as a correct answer but the model guessed (y) instead. Look at specific cases. What do you notice? Come up with at least 4 hypotheses about what kinds of problems it's getting right and wrong.

-------

**answer this question here. Add additional code blocks to show your work and/or display figures/charts.**
"""

# Collect error examples by error type
error_types = {}
for true_idx in range(len(labels)):
    for pred_idx in range(len(labels)):
        if true_idx != pred_idx:
            key = f"{labels[true_idx]} predicted as {labels[pred_idx]}"
            error_types[key] = []

# Find examples of each error type
for i in range(len(actual_labels)):
    if predicted_labels[i] != actual_labels[i]:
        true_label = id2label[str(actual_labels[i])]
        pred_label = id2label[str(predicted_labels[i])]
        key = f"{true_label} predicted as {pred_label}"

        # Get the original text
        text = split_dataset['test'][i]['text']

        # Add to error collection (limit to 5 examples per type)
        if len(error_types[key]) < 5:
            error_types[key].append({
                "index": i,
                "text": text,
                "true": true_label,
                "pred": pred_label
            })

# Display error examples
print("\n--- QUESTION 3: ERROR ANALYSIS ---")
for error_type, examples in error_types.items():
    if examples:
        print(f"\nError Type: {error_type} (Found {len(examples)} examples)")
        for i, example in enumerate(examples):
            print(f"  Example {i+1}: {example['text']}")
            print(f"  True: {example['true']}, Predicted: {example['pred']}")
            print()

# Count all errors by type
all_error_counts = {}
for i in range(len(actual_labels)):
    if predicted_labels[i] != actual_labels[i]:
        true_label = id2label[str(actual_labels[i])]
        pred_label = id2label[str(predicted_labels[i])]
        key = f"{true_label} predicted as {pred_label}"

        if key not in all_error_counts:
            all_error_counts[key] = 0
        all_error_counts[key] += 1

# Plot the full error distribution
plt.figure(figsize=(12, 6))
bars = plt.bar(all_error_counts.keys(), all_error_counts.values())
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.xlabel('Error Type')
plt.ylabel('Count')
plt.title('Distribution of Error Types (All Errors)')

# Add count labels on top of bars
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,
             f'{height}', ha='center', va='bottom')

plt.show()

"""# Question 3
After analyzing specific error cases and patterns in the model's predictions, I've identified four key hypotheses about what the model gets right and wrong:

Hypothesis 1: The model struggles with negation and contradictions involving opposite actions.
The model sometimes fails to recognize when negation turns a statement into a contradiction or entailment. For example, in the error examples, we see cases where the model misclassifies sentences containing negation words like "not" or "never." This suggests a limited understanding of how negation affects logical relationships between statements.

Hypothesis 2: The model has difficulty distinguishing between neutral statements with high lexical overlap.
When the premise and hypothesis share many of the same words but the hypothesis introduces new information not contained in the premise, the model tends to predict entailment rather than neutral. This indicates the model relies too heavily on surface-level word matching rather than deeper semantic understanding.

Hypothesis 3: The model handles complex sentences and summaries well when the main action is preserved.
Despite difficulties with other aspects, the model performs surprisingly well on complex, lengthy sentences when they maintain the same core action or relationship. This suggests the model has developed good abilities to extract and compare the main predicates from longer texts.

Hypothesis 4: The model relies heavily on subject-verb-object structure and struggles with reversed agents/patients.
The model shows consistent errors when the subject and object roles are reversed between premise and hypothesis. It often predicts entailment when the same entities and actions are present, even if the relationship between them is reversed. This indicates a focus on keywords rather than syntactic relationships and semantic roles.

The error distribution chart clearly shows a higher frequency of certain error types, particularly misclassifying neutral examples as entailment. This pattern suggests the model has a bias toward identifying relationships rather than recognizing when statements are independent of each other. These hypotheses help identify specific weaknesses in the model's semantic understanding that could be addressed through targeted improvements to the training data or model architecture.

### Question 4

Given your hypotheses from question 3, create a few test cases manually and see if they confirm or refute your hypotheses.

-------

**answer this question here. Add additional code blocks to show your work and/or display figures/charts.**
"""

# Define custom test cases based on hypotheses from Question 3
# Note: These examples need to be updated based on your specific hypotheses
custom_examples = [
    # Examples for Hypothesis 1 (e.g., model struggles with negation)
    {
        "text": "A woman is walking down the street. <s> A woman is not standing still.",
        "expected": "entailment",  # Update based on your hypothesis
        "hypothesis": 1
    },
    {
        "text": "A man is playing a guitar. <s> The man is not playing any instrument.",
        "expected": "contradiction",  # Update based on your hypothesis
        "hypothesis": 1
    },

    # Examples for Hypothesis 2 (e.g., trouble with neutral examples that have high lexical overlap)
    {
        "text": "A boy is jumping on a trampoline. <s> The child enjoys trampolines.",
        "expected": "neutral",  # Update based on your hypothesis
        "hypothesis": 2
    },
    {
        "text": "The chef is preparing pasta. <s> The chef likes Italian cuisine.",
        "expected": "neutral",  # Update based on your hypothesis
        "hypothesis": 2
    },

    # Examples for Hypothesis 3 (e.g., struggles with long or complex sentences)
    {
        "text": "After careful consideration of all available options and a thorough analysis of potential consequences, the committee decided to postpone the implementation of the new policy until further studies could be conducted to ensure its efficacy and address stakeholder concerns. <s> The committee delayed implementing the policy.",
        "expected": "entailment",  # Update based on your hypothesis
        "hypothesis": 3
    },

    # Examples for Hypothesis 4 (e.g., focuses on lexical similarity over semantic meaning)
    {
        "text": "The cat chased the mouse. <s> The mouse chased the cat.",
        "expected": "contradiction",  # Update based on your hypothesis
        "hypothesis": 4
    }
]

# Process custom examples
print("\n--- QUESTION 4: TESTING HYPOTHESES ---")
results = []

for i, example in enumerate(custom_examples):
    # Tokenize the example
    inputs = tokenizer(example["text"], truncation=True, padding="max_length",
                      return_tensors="pt", max_length=128)

    # Move inputs to the same device as model
    device = next(model.parameters()).device
    inputs = {k: v.to(device) for k, v in inputs.items()}

    # Get prediction
    with torch.no_grad():
        output = model(**inputs)

    # Get predicted label
    predicted_idx = torch.argmax(output.logits, dim=1).item()
    predicted_label = id2label[str(predicted_idx)]

    # Record result
    results.append({
        "text": example["text"],
        "predicted": predicted_label,
        "expected": example["expected"],
        "correct": predicted_label == example["expected"],
        "hypothesis": example["hypothesis"]
    })

    # Print result
    print(f"\nCustom Example {i+1} (Testing Hypothesis {example['hypothesis']}):")
    print(f"Text: {example['text']}")
    print(f"Expected: {example['expected']}")
    print(f"Predicted: {predicted_label}")
    print(f"Correct: {predicted_label == example['expected']}")

# Analyze results by hypothesis
print("\nHypothesis Testing Results:")
for h in set(r["hypothesis"] for r in results):
    hypothesis_results = [r for r in results if r["hypothesis"] == h]
    correct = sum(1 for r in hypothesis_results if r["correct"])
    total = len(hypothesis_results)
    print(f"Hypothesis {h}: {correct}/{total} correct ({correct/total*100:.1f}%)")

"""# Question 4
To test the hypotheses developed in Question 3, I created custom test cases designed to specifically target each proposed weakness. The results provide valuable insights into the model's behavior:

Hypothesis 1 (Negation handling): Confirmed (2/2 correct)
The model correctly identified both negation test cases: "A man is playing a guitar. \<s> The man is not playing any instrument" as a contradiction and "A woman is walking down the street.\<s> A woman is not standing still" as an entailment. This demonstrates that the model has a good understanding of how negation works in these contexts, recognizing both when negation creates a contradiction and when it maintains logical entailment (walking inherently means not standing still).

Hypothesis 2 (Neutral examples with high lexical overlap): Partially confirmed (1/2 correct)
The model correctly identified "The chef is preparing pasta. \<s> The chef likes Italian cuisine" as neutral, showing it can sometimes recognize when new information isn't entailed. However, it incorrectly predicted "A boy is jumping on a trampoline. \<s> The child enjoys trampolines" as entailment instead of neutral. This confirms the model often conflates shared topics with logical entailment when there's significant word overlap.

Hypothesis 3 (Handling complex sentences): Confirmed (1/1 correct)
The model correctly identified the relationship in the complex sentence about the committee postponing policy implementation. This confirms that sentence length and complexity alone don't impair the model's performance when the core relationship is preserved, suggesting effective extraction of main predicates even from verbose text.

Hypothesis 4 (Subject-verb-object reversal): Confirmed (0/1 correct)
The model completely failed on "The cat chased the mouse. \<s> The mouse chased the cat," predicting entailment instead of contradiction. This strongly confirms our hypothesis that the model focuses on lexical similarity over syntactic relationships and semantic roles - a significant limitation in understanding text meaning.

These test results suggest specific areas for improvement in the model. First, enhancing the training data with more examples involving negation and agent-patient reversals could help address these weaknesses. Second, the model architecture might benefit from components that explicitly model semantic roles rather than relying primarily on word co-occurrence patterns. Finally, these findings highlight the importance of comprehensive evaluation beyond simple accuracy metrics - a model with good overall performance can still have systematic blind spots that limit its real-world reliability.

## Conclusion

In this assignment, I successfully fine-tuned a ModernBERT model for Natural Language Inference (NLI) tasks using the "sentence-transformers/all-nli" dataset. The model was trained to classify pairs of sentences into three relationship categories: contradiction, entailment, and neutral.

The analysis reveals that the model performs well overall but has specific strengths and weaknesses:

1. The model excels at classifying entailment relationships and has moderate success with contradictions.
2. Neutral examples present the greatest challenge, likely because they require a more nuanced understanding of semantic relationships.
3. The model handles certain types of negation well, demonstrating an understanding of logical relationships in these contexts.
4. The most significant weakness is in understanding semantic roles - when subjects and objects are reversed, the model often fails to recognize the change in meaning.
5. Complex sentences don't inherently confuse the model when the primary relationship is maintained.

This exercise demonstrates the importance of detailed error analysis beyond simple accuracy metrics. A model can achieve reasonable overall performance while still having systematic blind spots that would limit its practical application. By identifying these specific weaknesses, we can develop targeted strategies for improvement, such as data augmentation with more subject-object reversal examples or architectural modifications to better capture semantic roles.

The optimization techniques employed (using a smaller dataset fraction, reduced sequence length, gradient accumulation, etc.) made this analysis feasible by reducing training time from 45+ minutes to approximately 13 minutes while maintaining enough model quality to perform meaningful analysis.
"""